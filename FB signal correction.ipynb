{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Backward and FISTA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will code the Forward-Backward aLgorithm and its inertial an accelerated version FISTA to solve several optimization problems associated for example to the denoising and the inpaiting problem. I provide two images but you can use one of your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scp\n",
    "import pylab as pyl\n",
    "import pywt\n",
    "import pandas as pd\n",
    "import holoviews as hv\n",
    "import param\n",
    "import panel as pn\n",
    "import requests\n",
    "import numpy.linalg as npl\n",
    "\n",
    "from panel.pane import LaTeX\n",
    "hv.extension('bokeh')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=0\n",
    "def chargeData(name):\n",
    "    if local:\n",
    "        if name=='Lenna':\n",
    "            res=np.array(Image.open(\"./Archive/img/Lenna.jpg\")).astype(float)\n",
    "        if name=='Canaletto':\n",
    "            res=np.array(Image.open(\"./Archive/img/Canaletto.jpeg\")).astype(float)\n",
    "        if name=='Minotaure':\n",
    "            res=np.array(Image.open(\"./Archive/img/MinotaureBruite.jpeg\")).astype(float)   \n",
    "        if name=='Cartoon':\n",
    "            res=np.array(Image.open(\"./Archive/img/Cartoon.jpg\")).astype(float) \n",
    "    else:\n",
    "        if name=='Lenna':\n",
    "            url='https://plmlab.math.cnrs.fr/dossal/optimisationpourlimage/raw/master/img/Lenna.jpg'        \n",
    "            response = requests.get(url)\n",
    "            res=np.array(Image.open(BytesIO(response.content))).astype(float)\n",
    "        if name=='Canaletto':\n",
    "            url='https://plmlab.math.cnrs.fr/dossal/optimisationpourlimage/raw/master/img/Canaletto.jpeg'\n",
    "            response = requests.get(url)\n",
    "            res=np.array(Image.open(BytesIO(response.content))).astype(float)\n",
    "        if name=='Minotaure':\n",
    "            url='https://plmlab.math.cnrs.fr/dossal/optimisationpourlimage/raw/master/img/MinotaureBruite.jpeg'\n",
    "            response = requests.get(url)\n",
    "            res=np.array(Image.open(BytesIO(response.content))).astype(float)\n",
    "        if name=='Cartoon':\n",
    "            url='https://plmlab.math.cnrs.fr/dossal/optimisationpourlimage/raw/master/img/Cartoon.jpg'        \n",
    "            response = requests.get(url)\n",
    "            res=np.array(Image.open(BytesIO(response.content))).astype(float)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=chargeData('Canaletto')\n",
    "im2=chargeData('Lenna')\n",
    "imagesRef= {\"Lenna\" : im2,\"Canaletto\" : im}\n",
    "options = dict(cmap='gray',xaxis=None,yaxis=None,width=400,height=400,toolbar=None)\n",
    "pn.Row(hv.Image(im).opts(**options),hv.Image(im2).opts(**options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(I,Iref):\n",
    "    temp=I.ravel()\n",
    "    tempref=Iref.ravel()\n",
    "    NbP=I.size\n",
    "    EQM=np.sum((temp-tempref)**2)/NbP\n",
    "    b=np.max(np.abs(tempref))**2\n",
    "    return 10*np.log10(b/EQM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inpainting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inpainting problem, we suppose we have data $y=Mx^0+b$ which are images of a target image $x^0$ through a Masking operator $M$ and we will consider also that data may be corrupted by an additional noise $b$. We want to estimate $x^0$ solving the following optimization problem \n",
    "\\begin{equation}\\label{Eq1}\n",
    "\\frac{1}{2}\\Vert Mx-y\\Vert^2+\\lambda \\Vert Tx\\Vert_1\n",
    "\\end{equation}\n",
    "where $T$ is an orthogonal transformation such that $Tx^0$ is sparse. If $x^0$ is piecewise smooth, we can use an orthogonal wavelet transform such as Daubechies 2 or 4. To perform the FB algorithm, we need to compute the gradient of $\\frac{1}{2}\\Vert Mx-y\\Vert^2$ and the proximal operator $\\lambda \\Vert Tx\\Vert_1$. Some help is provioded further in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the noise $b$ vanishes, i.e $y=Mx^0$ where $M$ is the masking operator, it may be better to solve the following optimization problem :\n",
    "\\begin{equation}\\label{Eq2}\n",
    "\\underset{x}{\\min}\\Vert Tx\\Vert_1\\text{ under the constraint }y=Mx  \n",
    "\\end{equation}\n",
    "This problem can't be solved directly using the FB algorithm since it can't be stated as the sum of a differentiable function which gradient is Lipschitz and of a function which proximal operator is known.\n",
    "\n",
    "Moreover, in any pratical problem, there is actually some noise, even it can be small.\n",
    "\n",
    "It can be shown that solutions of \\eqref{Eq1} converge to solutions of \\eqref{Eq2} when $\\lambda$ tends to 0. \n",
    "The advantage of \\eqref{Eq1} is that it can be solved using FB.\n",
    "Solving \\eqref{Eq1} allows to inpaint and to regularize (somehow remove some noise). The regularization effect grows with $\\lambda$. In many practical case, with small noise, $\\lambda$ must be chosen quiete small in \\eqref{Eq1}. \n",
    "The main drawback of such a choice is that it slows down the algorithm, an higher number of iterations is needed to get a good numerical result. When we use these methods, we must be aware that the choice of $\\lambda$ of the number of iterations and the starting point of this algorithm is essential. If the numerical resultats are not as nice as you wish, you may have to change the value of $\\lambda$ or increase the number of iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few known results on explicit Gradient Descent and Forward-backward (FB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $F=f+g$ is a convex fucntion, sum of two convex functions, $f$ a differentiable function which gradient is $L-$Lipschitz and $g$ a convex function such that $prox_g$ is known, the Forward-Backward algorithm is defined by \n",
    "$$x_{n+1}=prox_{hg}(x_n-h\\nabla f(x_n))=Tx_n\\quad \\text{ with }T:=prox_{hg}\\circ (Id-h\\nabla f)$$\n",
    "One can show that the sequence $(F(x_n)-F(x^*))_{n\\in\\mathbb{N}}$ is non increasing and moreover \n",
    "$$F(x_n)-F(x^*)\\leqslant \\frac{2\\Vert x_0-x^*\\Vert^2}{hn}$$\n",
    "This decay rate $\\frac{1}{n}$ is optimal in the sens that it is impossible to get a bound decaying like $\\frac{1}{n^{\\delta}}$ with $\\delta>1$ for all convex fucntions. Nevertheless it can be proved that if $h<\\frac{1}{L}$ one actually has\n",
    "$$F(x_n)-F(x^*)=o\\left(\\frac{1}{n}\\right).$$  \n",
    "If the function $f$ is strongly convex the decay is geometrical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some rermarks on iteratives algorithms and FB in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use an iterative algorithm depending on one or more parameters, you must be careful to evaluate the results, the outputs of the algorithm. It is crucial to check some points :\n",
    "\n",
    "1) Check that the algorithm is close to converge... that is check that enough iterations have been done. This number of iterations may change depending on parameters and on the starting point $x_0$ chosen to build ythe sequence.\n",
    "\n",
    "To check this point, I Invite you to display the curve of the values $F(x_n)$ and to think twice about what you doing and what you see on the outputs. For example, if you are performing inpainting and if some black pixels left, or if the sequence $F(x_n)$ is still decaying quite fast, it means that it may be relevant to increse the number of iterations. More generally, observing the artefacts of defects on the output may give some clues of what can be done to improve the result. Moreover, have a look to the sequence $F(x_n)$ may indicate that less iterations could have been performed to get a similar output.\n",
    "\n",
    "2) Explore the possible values of paramters. It may occur that some values of parameters provide some relevant results and some don't. Before pretending that an algorithm is not efficient you must ensure that the parameters have been chosen wisely. These parameters can be the ones of the fucntion to minimize such as $\\lambda$ in \\eqref{Eq1} or the internal parameters of the algorithm such as the descent step for example. Making an Experiments Plan may be a good way to estimate the values of good parameters.\n",
    "\n",
    "One can observe that the value of $F(x)$ is not the only criterion to evaluate the output of an algorithm, actually for different values of $\\lambda$, the PSNR may be more relevant. We can't avoid thinking... \n",
    "\n",
    "3) When it it is possible choose a good starting point. Iterative algorithms build images (or signals) which converges to a minimiozer of the fucntion $F$. the choice of the staring point may be crucial. \n",
    "If we do not have any information, we can choose the constant image 0 or any relevant image, for example the masked image if we are performing inapainting or the noisy image if we are performing denoising. If for any problem a simple and quick method may provide a rough or innacurate output, it can be used as a starting point of the method. For the inpainting, we may start using a simple median filtering. The image is divided into small squares and the holes are filled up with the median value of the observed data.\n",
    "\n",
    "One can often distinguish two steps during the optimization of the function $F$, a first one to go from the starting point to a good image, a coarse approximation of the whished output and a second one from the coarse approximation to the whished output. It may be difficult to avoid the second step but the first one may be shortened using a good starting point.\n",
    "\n",
    "In many situations to compare several methods with various parameters or wavelet bases, it can be useful to compute the PSNR of each output on a benchmark to get a quite fair criterion for the comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Backward for the inpainting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start using observations without any noise and apply a mask. To create a 2D mask one can use the following commands : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "n1,n2=np.shape(im)\n",
    "r=np.random.rand(n1,n2)\n",
    "M=(r<0.5)\n",
    "M=M*1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can of course change the proportion of pixels that are masked.\n",
    "\n",
    "We can mask the image by the following command :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=M*im\n",
    "pn.Row(hv.Image(im).opts(cmap='gray',width=400,height=400,title=\"Image originale\"),\n",
    "       hv.Image(temp).opts(cmap='gray',width=400,height=400,title=\"Image masquée\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilisation by median filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and test a median filter which inpaint roughly the image replacing missing pixels using the median of values of observed pixels on vois*vois squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FiltreMedian(im,vois) : \n",
    "    imrec=np.copy(im)\n",
    "    n1,n2=np.shape(im)\n",
    "    K1=int(np.floor(n1/vois))\n",
    "    K2=int(np.floor(n2/vois))  \n",
    "    for k1 in range (K1) : \n",
    "        for k2 in range (K2) : \n",
    "            voisinage = np.copy(im[k1*vois:(k1+1)*vois, k2*vois:(k2+1)*vois])\n",
    "            med_vois = np.median(voisinage[voisinage > 0])\n",
    "            for v1 in range(vois) : \n",
    "                for v2 in range (vois) : \n",
    "                    if voisinage[v1,v2]== 0 : \n",
    "                        voisinage[v1,v2]=med_vois\n",
    "            imrec[k1*vois:(k1+1)*vois, k2*vois:(k2+1)*vois] = voisinage\n",
    "    return(imrec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imfiltre = FiltreMedian(temp, 16)\n",
    "pn.Row(hv.Image(temp).opts(cmap='gray', xaxis=None, yaxis=None, width=350, height=350, title=\"Image masquée\"), \n",
    "       hv.Image(imfiltre).opts(cmap='gray',xaxis=None,yaxis=None, width=350, height=350, title=\"Image FiltreMedian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to FB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function computing the gradient of the function $f(x)=\\frac{1}{2}\\Vert Mx-y\\Vert^2$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M est un projeté orthogonal donc $M^\\top=M=M^2$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla f(x) &= M^\\top (M x - y) \\\\\n",
    "            &= M^\\top Mx - M^\\top y \\\\\n",
    "            &= Mx - My \\\\\n",
    "            &= M(x-y)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientInpainting(x,b,M):\n",
    "    g=M*(x-b)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For which value of $L$ this gradient is $L-$Lipschitz ?\n",
    "\n",
    "* On projette sur l'espace vectoriel des pixels non nuls donc M est un projecteur orthogonal.\n",
    "* De plus la norme d'opérateur est égale à 1.\n",
    "* Donc L=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions compute the proximal operator of the function $g(x)=\\lambda \\Vert Tx\\Vert_1$ where $T$ is an orthogonal wavelet transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeuillageDouxOndelettes(I,wave,Seuil):\n",
    "    L=pywt.dwt_max_level(len(I),pywt.Wavelet(wave).dec_len)\n",
    "    wavelet_coeffs= pywt.wavedecn(I, wave, mode='per', level=L)\n",
    "    arr, coeff_slices, coeff_shapes = pywt.ravel_coeffs(wavelet_coeffs)\n",
    "    temp=pywt.threshold(arr,Seuil,mode='soft')\n",
    "    test=pywt.unravel_coeffs(temp, coeff_slices, coeff_shapes, output_format='wavedecn')\n",
    "    Irec=pywt.waverecn(test, wave,mode='per')\n",
    "    return Irec\n",
    "\n",
    "def Normel1Ondelettes(I,wave):\n",
    "    L=pywt.dwt_max_level(len(I),pywt.Wavelet(wave).dec_len)\n",
    "    wavelet_coeffs= pywt.wavedecn(I, wave, mode='per', level=L)\n",
    "    arr, coeff_slices, coeff_shapes = pywt.ravel_coeffs(wavelet_coeffs)\n",
    "    norml1=sum(np.abs(arr))\n",
    "    return norml1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave='haar'\n",
    "imrec=SeuillageDouxOndelettes(im,wave,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and test an inpainting program using the FB algorithm minimizing the function \\eqref{Eq1}.\n",
    "Don't forget to clip the data at the end of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardBackwardInpaintingv2(y,M,step,lam,Niter,wave):\n",
    "    seuil=lam*step\n",
    "    xn=FiltreMedian(y,8)\n",
    "    F=np.zeros(Niter)\n",
    "    for i in range(Niter):\n",
    "        grad = GradientInpainting(xn,y,M)\n",
    "        xn_new  = xn - step * grad\n",
    "        xn  = SeuillageDouxOndelettes(xn_new, wave, seuil)\n",
    "        F[i] = (1/2)* npl.norm(M * xn - y)**2 + lam * Normel1Ondelettes(xn_new,wave)\n",
    "    return np.clip(xn_new, 0, 255), F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Im_rec, F = ForwardBackwardInpaintingv2(temp, M, 0.1, 0.1, 100, 'haar')\n",
    "hv.Curve(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire :\n",
    "\n",
    "Notre but étant de minimiser F, mission accomplie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a dashboard allowing a numerical exploration of the previous function. We could observe that small small values of $\\lambda$ produce better output. Theoreticaly, if the data are noiseless, it would be better to take $\\lambda$ as small as possible. You can see it if you display the PSNR of the output when you compute a number of iterations that is large enough.\n",
    "\n",
    "The drawback of such a choice is that it slows down extremely the algorithm. If data are noisy the optimal value of $\\lambda$ is proportional to the standart deviation of the noise.\n",
    "\n",
    "Hence, you may prefer a small value of $\\lambda$ but nottoo small to ensure the convergence of the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelist = ['haar','db2','db3','db4','coif1','coif2','coif3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBInpaint(param.Parameterized):\n",
    "    wave = param.ObjectSelector(default=\"haar\",objects=wavelist)\n",
    "    image = param.ObjectSelector(default=\"Canaletto\",objects=imagesRef.keys())\n",
    "    Niter = param.Integer(10,bounds=(0,300))\n",
    "    lam = param.Number(10,bounds=(1,30))\n",
    "    step = param.Number(0.9,bounds=(0.5,4))\n",
    "    masquage = param.Number(0.5,bounds=(0.1,1))\n",
    "    def view(self):\n",
    "        Im = imagesRef[self.image]\n",
    "        Affich_ref = hv.Image(Im).opts(cmap='gray',width=400,height=400,title=\"Image originale\")\n",
    "        \n",
    "        n1, n2 = np.shape(Im)\n",
    "        r = np.random.rand(n1, n2)\n",
    "        M = (r > self.masquage) * 1.0\n",
    "        Im_masq = Im * M\n",
    "        Affich_masq = hv.Image(Im_masq).opts(cmap='gray',width=400,height=400,title=\"Image masquée\")\n",
    "        psnr_masq = PSNR(Im_masq,Im)\n",
    "        \n",
    "        Im_rec, F = ForwardBackwardInpaintingv2(Im_masq, M, self.step, self.lam, self.Niter, self.wave)\n",
    "        Affich_rec = hv.Image(Im_rec).opts(cmap='gray',width=400,height=400,title=\"Image reconstruite\")\n",
    "        psnr_rec = PSNR(Im_rec,Im)\n",
    "        \n",
    "        strp1 = \"%2.2f\" % psnr_masq\n",
    "        te1='PSNR image masquée = '\n",
    "        TN1=hv.Text(0,0.2,te1+strp1).opts(xaxis=None,yaxis=None,toolbar=None)\n",
    "        strp2 = \"%2.2f\" % psnr_rec\n",
    "        te2='PSNR image reconstruite = '\n",
    "        TN2=hv.Text(0,0,te2+strp2).opts(xaxis=None,yaxis=None)\n",
    "        \n",
    "        return pn.Row(pn.Column(Affich_ref,Affich_masq),pn.Column(Affich_rec,TN1*TN2))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbinpaint=FBInpaint()\n",
    "pn.Row(fbinpaint.param,fbinpaint.view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaires :\n",
    "\n",
    "* Si on prend un lambda petit alors on résout le bon problème d'optimisation et on obtient un très bon PSNR.\n",
    "* En revanche, il ne faut pas qu'il soit trop petit sinon on aura besoin d'un trop grand nombre d'itérations pour converger.\n",
    "* Si le nombre d'itération est trop faible, l'algorithme n'aura pas le temps de converger et les résultats obtenus ne seront pas pertinents.\n",
    "* Il faut donc ajuster lambda de telle manière à trouver le bon équilibre entre précision et rapidité : lambda potentiellement petit, mais pas trop, avec un nombre d'itérations raisonnable.\n",
    "\n",
    "* Le pas mesure la vitesse à laquelle on va \"descendre\" vers la solution.\n",
    "* Si le pas est trop grand, il y a un risque de dépasser le minimum et de diverger ou alors d'osciller autour du minimum.\n",
    "* Si le pas est trop petit, alors l'algo aura besoin de trop d'itérations pour converger.\n",
    "* Il faut donc trouver un pas ni trop grand ni trop petit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accélération de Nesterov, FISTA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yurii Nesterov has proposed in the 80's several methods to accelerate the explicit Gradient Descent. We will focus on one of these, the one that is described in the lesson. If you have a look to research paper be aware that the 2 words \"Nesterov accelerations\" may have several meanings... using interpolation or extrapolation, be specific to strongly convex functions or not. it may apply to the explicit Gradient Descent (GD) or to the Forward Backward algorithm. Moreover we will not use the original parameter of Nesterov or FISTA but the ones we proposed in 2014 with Antonin Chambolle to ensure the convergence of iterates and speed up the convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the acceleration of the GD proposed but Nesterov in 19!4 and adapted to FB under the name of FISTA by Beck and Teboulle in 2009 is easy the apply and without any additional computational cost.\n",
    "\n",
    "The idea is to apply the FB (or the GD for a single differntiable function) to a shifted point of $x_n$ with a step $h<\\frac{1}{L}$. \n",
    "Be careful this bound is more restrictive than the one for the classical GD :\n",
    "$$x_{n+1}=T(x_n+\\alpha_n(x_n-x_{n-1}))$$ \n",
    "where the sequence $\\alpha_n$ is chosen in a suitable way and $T$ is the FB operator. We say that FISTA is an inertial method because it uses an inertial term as a memory of the last descent direction.\n",
    "\n",
    "Let's give some key points of this method :\n",
    "1) The original choice of Nesterov for the sequence $\\alpha_n$ is the following :\n",
    "\\begin{equation}\n",
    "\\alpha_n=\\frac{t_n-1}{t_{n+1}}\\text{ avec }t_1=1\\text{ et }t_{n+1}=\\frac{1+\\sqrt{1+t_n^2}}{2}\n",
    "\\end{equation}\n",
    "2) For this choice we have \n",
    "$$F(x_n)-F(x^*)\\leqslant \\frac{2\\Vert x_0-x^*\\Vert^2}{hn^2}$$\n",
    "3) We can take the more simple choice $\\alpha_n=\\frac{n-1}{n+a-1}$ with $a>3$ (and that's what you will implement) and in this cas we have \n",
    "$$F(x_n)-F(x^*)\\leqslant \\frac{(a-1)^2\\Vert x_0-x^*\\Vert^2}{2h(n+a)^2}$$ and moreover \n",
    "$F(x_n)-F(x^*)=o\\left(\\frac{1}{n^2}\\right)$ and the sequence $(x_n)_{n\\geqslant 1}$ converges. \n",
    "It can be noticed, there are no inertia in the first step ($\\alpha_1=0$) and thus $x_1=T(x_0)$. \n",
    "The inertia appears for the computation of $x_2$. The original choice of Nesterov is very close to the choice $\\alpha=3$.\n",
    "\n",
    "4) In the case of a composite fucntion ($F$ is a sum of a diffrentiable fucntion $f$ and a possibly non smooth fucntion $g$), this inertial algorithm is called FISTA (Fast Iterative Soft Shrinkage Thresholding Algorithm) because when $g$ is the $\\ell_1$ norm, the proximal operator is a soft thresholding but the name FISTA is the name of this algorithm even if $g$ is not an $\\ell_1$ norm.\n",
    "\n",
    "5) Unlike FB, the sequence $(F(x_n)-F(x^*))_{n\\geqslant 1}$ given by FISTA is not necessarily non increasing. The previous bounds on FB or FISTA are only ... bounds... but does not describe the real decay rate.\n",
    "In practical cases you should see that FISTA is often faster than FB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an inpaiting algorithm minimizing the fucntion \\eqref{Eq1} using FISTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FISTAInpainting(y,M,step,lam,Niter,wave,alpha0):\n",
    "    seuil=lam*step\n",
    "    xn=FiltreMedian(y,8)\n",
    "    F=np.zeros(Niter)\n",
    "    xn_old=xn\n",
    "    for k in np.arange(0,Niter):\n",
    "        alpha = k/(k+alpha0)\n",
    "        xn_new = xn + alpha*(xn - xn_old)\n",
    "        grad = GradientInpainting(xn_new,y,M)\n",
    "        xn_old = xn\n",
    "        xn = xn_new - step * grad \n",
    "        xn = SeuillageDouxOndelettes(xn, wave, seuil)\n",
    "        F[k] = (1/2)* npl.norm(M * xn - y)**2 + lam * Normel1Ondelettes(xn,wave)\n",
    "    return np.clip(xn,0,255),F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FISTAInpaint(param.Parameterized):\n",
    "    wave = param.ObjectSelector(default=\"haar\",objects=wavelist)\n",
    "    image = param.ObjectSelector(default=\"Canaletto\",objects=imagesRef.keys())\n",
    "    Niter = param.Integer(10,bounds=(0,500))\n",
    "    lam = param.Number(10,bounds=(0.01,10))\n",
    "    step = param.Number(0.9,bounds=(0.5,4))\n",
    "    masquage = param.Number(0.5,bounds=(0.1,1))\n",
    "    alpha = param.Number(3,bounds=(1,10))\n",
    "    def view(self):\n",
    "        Im = imagesRef[self.image]\n",
    "        Affich_ref = hv.Image(Im).opts(cmap='gray',width=400,height=400,title=\"Image originale\")\n",
    "        \n",
    "        n1, n2 = np.shape(Im)\n",
    "        r = np.random.rand(n1, n2)\n",
    "        M = (r > self.masquage) * 1.0\n",
    "        Im_masq = Im * M\n",
    "        Affich_masq = hv.Image(Im_masq).opts(cmap='gray',width=400,height=400,title=\"Image masquée\")\n",
    "        psnr_masq = PSNR(Im_masq,Im)\n",
    "        \n",
    "        Im_rec, f = FISTAInpainting(Im_masq, M, self.step, self.lam, self.Niter, self.wave, self.alpha)\n",
    "        Affich_rec = hv.Image(Im_rec).opts(cmap='gray',width=400,height=400,title=\"Image reconstruite\")\n",
    "        psnr_rec = PSNR(Im_rec,Im)\n",
    "        \n",
    "        strp1 = \"%2.2f\" % psnr_masq\n",
    "        te1='PSNR image masquée = '\n",
    "        TN1=hv.Text(0,0.2,te1+strp1).opts(xaxis=None,yaxis=None,toolbar=None)\n",
    "        strp2 = \"%2.2f\" % psnr_rec\n",
    "        te2='PSNR image reconstruite = '\n",
    "        TN2=hv.Text(0,0,te2+strp2).opts(xaxis=None,yaxis=None)\n",
    "        \n",
    "        return pn.Row(pn.Column(Affich_ref,Affich_masq),pn.Column(Affich_rec,TN1*TN2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fistainpaint=FISTAInpaint()\n",
    "pn.Row(fistainpaint.param,fistainpaint.view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaires :\n",
    "L'algorithme de Nesterov est un cas particulier de l'accélération de Nesterov avec $\\alpha$=3. Nous remarquons que les résultats sont meilleurs pour $\\alpha$>3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A retenir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implement a FB algorithm, nesterov accelerations are often efficient and it is always useful to test and even to test several choices of $a$. beginning with $a=3$. One can notive that, at least theoretically, the sequence   \n",
    "generated $(F(x_n)-F(x^*))_{n\\geqslant 1}$ generated by FB as a geometrical decay when $F$ is strongly convex, which is not the case for the studied Nesterov scheme. But it exists some accelerations that are dedicated to this specific situation. In practical cases, for a reasonnable number of iterations, these Nesterov accelerations are really competitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
